# 异步协程专题

#### 简单熟悉一下语法

```python
import asyncio
import time
async def func1():
	print("第一个函数执行")
	time.sleep(5)
	print("第一个函数再次执行")
async def func2():
	print("第二个函数执行")
	time.sleep(5)
	print("第二个函数再次执行")
async def func3():
	print("第三个函数执行")
	time.sleep(5)
	print("第三个函数再次执行")            
if __name__=='__main__':
    f1=func1()
    f2 =func2()
    f3 =func3()
    tasks=[f1,f2,f3]
    asyncio.run(asyncio.wait(tasks)) #有多个任务同时执行的情况，必须把任务交给				                                 #asyncio.wait()函数进行执行，而启动函数wait需要用                                      #run来启动
```

但是这种异步操作跑出来的却是同步的效果，原因在于里面有sleep操作，sleep操作是一个同步的操作，当异步执行到同步的操作，**异步的操作就终止了**

```python 
将sleep改为await asycio.sleep就可以执行异步操作的
import asyncio
import time
async def func1():
    print("第一个函数执行")
    await asyncio.sleep(3)
    print("第一个函数再次执行")

async def func2():
    print("第二个函数执行")
    await asyncio.sleep(5)
    print("第二个函数再次执行")

async def func3():
    print("第三个函数执行")
    await asyncio.sleep(2)
    print("第三个函数再次执行")
if __name__=='__main__':
    f1=func1()
    f2 =func2()
    f3 =func3()
    tasks=[f1,f2,f3]
    start = time.time()
    asyncio.run(asyncio.wait(tasks))
    stop = time.time()
    print(stop-start)

这时候同步操作为九秒的操作就变为了五秒
5.000604152679443
```

```python
   #这种写法是推荐的写法，就是多个协程对象丢到一个表里，然后统一await
    f1=func1()
    f2 =func2()
    f3 =func3()
    tasks=[f1,f2,f3]
    start = time.time()
    asyncio.run(asyncio.wait(tasks))
#这种写法不推荐
	f1=func1()
    await f1 #一般await挂起操作写在协程对象前面
```

#### 协程在爬虫中的运用框架

```python
async def download(url):
    print("准备开始下载")
    await asyncio.sleep(2)   #模拟网络下载任务
    print("下载完成")
async def main():
    urls =[
        "https://www.baidu.com",
        "https://163.com",
        "https://www.baidu.com"
    ]
    tasks=[]                 #做一个列表用于储存异步对象
    for url in urls:
        d=download(url)      #创建一个协程对象
        tasks.append(d)
    await asyncio.wait(tasks)#开始执行异步操作
                             #await的意思是当执行这个操作的时候挂起，去执行其他任务
if __name__ = '__main__':
    asyncio.run(main())
```

总结一下：需要有一个函数提供url，需要一个下载函数，写一个循环，将url一个个带入到下载函数里得到异步对象，循环结束，将任务挂起，开始异步操作

#### aiohttp和aiofiles

这两个函数的作用是代替file和request函数，因为file和request库不支持异步操作，使用这两个库来进行代替

#### **用异步的思想写一个简单的思路程序**

```python
import aiohttp
import aiofiles
import asyncio
header = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36 Edg/96.0.1054.43"}
urls = ["https://www.youmeitu.com/weimeitupian/144258.html",
        "https://www.youmeitu.com/weimeitupian/144249.html",
        "https://www.youmeitu.com/weimeitupian/144248.html"]

async def aio_download(url):
    name=url.rsplit("/",1)[1]
    #发送请求，得到图片内容，保存到文件
    async with aiohttp.ClientSession() as session:#这个约等于request模块
        async with session.get(url) as resp:
            with open(name,mode="wb") as f:
                f.write(await resp.content.read())#读取内容是异步的需要await挂起
    print("结束！")
async def main():
    tasks=[]
    for url in urls:
        tasks.append((aio_download(url)))
    await asyncio.wait(tasks)      #这种写法等于asyncio.run(asyncio.wait(tasks))
    pass
if __name__=='__main__':
    asyncio.run(main())
```

#### 异步实战

##### 爬取西游记

先查看页面源代码，异常的干净，说明一切都是后期渲染的，所以点击检查看发送了什么网络请求

发现请求了这个url：http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"4306063500"}得到了小说的章节名称，cid，收费情况

在这个url中http://dushu.baidu.com/api/pc/getChapterContent?data={"book_id":"4306063500","cid":"4306063500|1569782244","need_bookinfo":1}

得到了第一章的内容

简单分析，第一个url只是拿到章节名等其他信息，只需要请求一次，第二个url需要执行很多很多次，所以采取异步操作

```python
1.同步操作：访问getCatalog，拿到所有章节的cid和名称
2.异步操作：访问getChapterContent 下载所有章节内容
```

```python
import json
import aiofiles
import requests
import asyncio
import aiohttp
import os
header = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.55 Safari/537.36 Edg/96.0.1054.43"}
b=os.getcwd()
def display():
    b_id=input("输入要下载的书的book_id>")
    return b_id
async def aio_download(bookid,cid,title):
    data = {
        "book_id":f"{bookid}",
        "cid":f"{bookid}|{cid}",
        "need_bookinfo":1}
    data=json.dumps(data)
    url = f'http://dushu.baidu.com/api/pc/getChapterContent?data={data}'
    async with aiohttp.ClientSession() as session:
        async with session.get(url,headers=header) as resp:
            dic = await resp.json()                 #已经得到了小说页面的json内容
            book_name = dic['data']['bookinfo']['book_name']
            dic = dic['data']['novel']['content']   #拿到了小说的内容  data novel content
            if not os.path.exists(f"{b}/{book_name}"):
                os.mkdir(f"{b}/{book_name}")
            #接下来异步打开文件。写入文件
            async with aiofiles.open(f"{b}/{book_name}/{title}.txt",mode="w",encoding="utf-8") as f:
                await f.write(dic)

async def getCatalog(book_url,b_id):
    resp = requests.get(book_url,headers=header)
    dic = resp.json()
    tasks=[]
    for i in dic['data']['novel']['items']:
        title = i['title']
        cid = i['cid']
        tasks.append(aio_download(b_id,cid,title))
    await asyncio.wait(tasks)
if __name__=='__main__':
    b_id = str(display())
    book_url = 'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"'+b_id+'"}' #如果写成 url = f'http://dushu.baidu.com/api/pc/getCatalog?data={"book_id":"{b_id}}"}'那么这个自带的大括号也会被进行转义，所以采取另一种写法
    asyncio.run(getCatalog(book_url,b_id))

```

到这里一步小说西游记就爬取完成

